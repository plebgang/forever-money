"""
Main entry point for SN98 Validator (Jobs-Based Architecture).

Supports:
- Multiple concurrent jobs
- Dual-mode operation (evaluation + live)
- Reputation-based scoring
- Miner activity tracking
- Async/await with Tortoise ORM
- Rebalance-only protocol
"""
import argparse
import asyncio
import logging
import sys

import bittensor as bt
from web3 import AsyncHTTPProvider, AsyncWeb3

from validator.repositories.job import JobRepository
from validator.models.job import init_db, close_db
from validator.round_orchestrator import AsyncRoundOrchestrator
from validator.utils.env import (
    NETUID,
    SUBTENSOR_NETWORK,
    EXECUTOR_BOT_URL,
    EXECUTOR_BOT_API_KEY,
    REBALANCE_CHECK_INTERVAL,
    JOBS_POSTGRES_HOST,
    JOBS_POSTGRES_PORT,
    JOBS_POSTGRES_DB,
    JOBS_POSTGRES_USER,
    JOBS_POSTGRES_PASSWORD,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("validator.log"), logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)


def get_config():
    """Load configuration from environment and arguments."""
    parser = argparse.ArgumentParser(description="SN98 ForeverMoney Validator")

    # Wallet arguments
    parser.add_argument("--wallet.name", type=str, required=True, help="Wallet name")
    parser.add_argument(
        "--wallet.hotkey", type=str, required=True, help="Wallet hotkey"
    )

    # Network arguments
    parser.add_argument(
        "--subtensor.network",
        type=str,
        default=None,
        help=f"Subtensor network endpoint (e.g., ws://127.0.0.1:9944, wss://entrypoint-finney.opentensor.ai:443, or finney/test/local). Default: {SUBTENSOR_NETWORK}",
    )
    parser.add_argument(
        "--netuid",
        type=int,
        default=None,
        help=f"Network UID. Default: {NETUID}",
    )

    args = parser.parse_args()

    # All other config from environment, with CLI overrides
    config = {
        "netuid": args.netuid if args.netuid is not None else NETUID,
        "subtensor_network": getattr(args, "subtensor.network") or SUBTENSOR_NETWORK,
        "wallet_name": getattr(args, "wallet.name"),
        "wallet_hotkey": getattr(args, "wallet.hotkey"),
        "executor_bot_url": EXECUTOR_BOT_URL,
        "executor_bot_api_key": EXECUTOR_BOT_API_KEY,
        "rebalance_check_interval": REBALANCE_CHECK_INTERVAL,
    }

    # Build Tortoise DB URL from environment
    config[
        "tortoise_db_url"
    ] = f"postgres://{JOBS_POSTGRES_USER}:{JOBS_POSTGRES_PASSWORD}@{JOBS_POSTGRES_HOST}:{JOBS_POSTGRES_PORT}/{JOBS_POSTGRES_DB}"

    return config


async def run_jobs_validator(config):
    """
    Run validator in jobs-based mode with concurrent job execution.

    Uses async/await with Tortoise ORM and rebalance-only protocol.

    Args:
        config: Configuration dictionary
    """
    logger.info("=" * 80)
    logger.info("STARTING SN98 VALIDATOR (ASYNC JOBS-BASED ARCHITECTURE)")
    logger.info("=" * 80)

    # Initialize Bittensor components
    wallet = bt.Wallet(name=config["wallet_name"], hotkey=config["wallet_hotkey"])
    subtensor = bt.Subtensor(network=config["subtensor_network"])
    metagraph = subtensor.metagraph(netuid=config["netuid"])
    dendrite = bt.Dendrite(wallet=wallet)

    logger.info(f"Wallet: {wallet.hotkey.ss58_address}")
    logger.info(f"Network: {config['subtensor_network']}")
    logger.info(f"Netuid: {config['netuid']}")
    logger.info(f"Protocol: Rebalance-only (no StrategyRequest)")

    # Initialize Tortoise ORM
    logger.info("Initializing Tortoise ORM...")
    await init_db(config["tortoise_db_url"])
    logger.info("Database connected")

    # Initialize async job manager
    job_repository = JobRepository()
    logger.info("Async job manager initialized")

    # Initialize async round orchestrator
    orchestrator = AsyncRoundOrchestrator(
        job_repository=job_repository,
        dendrite=dendrite,
        metagraph=metagraph,
        config=config,
    )
    logger.info("Async round orchestrator initialized")

    # Track running jobs and their tasks
    running_jobs = {}  # job_id -> task

    logger.info("=" * 80)
    logger.info("Starting continuous job execution with dynamic job discovery...")
    logger.info("=" * 80)

    async def monitor_and_run_jobs():
        """Continuously monitor for new jobs and start them."""
        check_interval = 60  # Check for new jobs every 60 seconds

        while True:
            try:
                # Get all active jobs from database
                active_jobs = await job_repository.get_active_jobs()

                if not active_jobs:
                    logger.warning(
                        "No active jobs found. Waiting for jobs to be added..."
                    )
                    await asyncio.sleep(check_interval)
                    continue

                # Check for new jobs
                for job in active_jobs:
                    if job.job_id not in running_jobs:
                        logger.info(
                            f"NEW JOB DETECTED: {job.job_id} | "
                            f"Vault: {job.sn_liquditiy_manager_address} | "
                            f"Pair: {job.pair_address} | "
                            f"Round Duration: {job.round_duration_seconds}s"
                        )

                        # Start new task for this job
                        task = asyncio.create_task(
                            orchestrator.run_job_continuously(job),
                            name=f"job_{job.job_id}",
                        )
                        running_jobs[job.job_id] = task

                        logger.info(f"Started orchestration for job {job.job_id}")

                # Check for inactive jobs (jobs that were removed or deactivated)
                current_job_ids = {job.job_id for job in active_jobs}
                removed_jobs = set(running_jobs.keys()) - current_job_ids

                for job_id in removed_jobs:
                    logger.info(f"Job {job_id} is no longer active, cancelling task")
                    running_jobs[job_id].cancel()
                    del running_jobs[job_id]

                # Log status
                logger.info(
                    f"Currently running {len(running_jobs)} jobs: {list(running_jobs.keys())}"
                )

                await asyncio.sleep(check_interval)

            except Exception as e:
                logger.error(f"Error in job monitor: {e}", exc_info=True)
                await asyncio.sleep(check_interval)

    try:
        # Run the job monitor
        await monitor_and_run_jobs()

    except KeyboardInterrupt:
        logger.info("\n" + "=" * 80)
        logger.info("Keyboard interrupt received. Shutting down validator...")
        logger.info("=" * 80)

    finally:
        # Cancel all running job tasks
        logger.info(f"Cancelling {len(running_jobs)} running job tasks...")
        for job_id, task in running_jobs.items():
            logger.info(f"Cancelling task for job {job_id}")
            task.cancel()

        # Wait for all tasks to be cancelled
        if running_jobs:
            await asyncio.gather(*running_jobs.values(), return_exceptions=True)

        # Cleanup Tortoise ORM
        await close_db()
        logger.info("Database connections closed")


def main():
    """Main validator entry point."""
    try:
        config = get_config()
        asyncio.run(run_jobs_validator(config))
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
